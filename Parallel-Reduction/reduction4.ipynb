{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMPaIbZ2LnF3",
        "outputId": "d2aa75db-f63d-4fb6-fe3b-88e56c963f49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing four.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile four.cu\n",
        "#include <iostream>\n",
        "#include<cuda_runtime.h>\n",
        "#include <chrono>\n",
        "#include <numeric>\n",
        "\n",
        "// Adding this function to help with unrolling\n",
        "__device__ void warpReduce(volatile int* sdata, int tid){\n",
        "  // the aim is to save all the warps from useless work\n",
        "  sdata[tid] += sdata[tid + 32];\n",
        "  sdata[tid] += sdata[tid + 16];\n",
        "  sdata[tid] += sdata[tid + 8];\n",
        "  sdata[tid] += sdata[tid + 4];\n",
        "  sdata[tid] += sdata[tid + 2];\n",
        "  sdata[tid] += sdata[tid + 1];\n",
        "}\n",
        "\n",
        "// REDUCTION 4 â€“ Unroll Last Warp\n",
        "__global__ void reduce4(int *g_in_data, int *g_out_data){\n",
        "    extern __shared__ int sdata[];  // stored in the shared memory\n",
        "\n",
        "    // Each thread loading one element from global onto shared memory\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x*(blockDim.x*2) + threadIdx.x;\n",
        "    sdata[tid] = g_in_data[i] + g_in_data[i+blockDim.x];\n",
        "    __syncthreads();\n",
        "\n",
        "    // Reduction method -- occurs in shared memory\n",
        "    for(unsigned int s = blockDim.x/2; s > 32; s >>= 1){  // only changing the end limit\n",
        "        // check out the reverse loop above\n",
        "        if (tid < s){   // then, we check tid to do our computation\n",
        "            sdata[tid] += sdata[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Adding this to use warpReduce\n",
        "    if (tid < 32){\n",
        "      warpReduce(sdata, tid);\n",
        "    }\n",
        "\n",
        "    if (tid == 0){\n",
        "        g_out_data[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// I hope to use this main file for all of the reduction files\n",
        "int main(){\n",
        "    int n = 1<<22; // Increase to about 4M elements\n",
        "    size_t bytes = n * sizeof(int);\n",
        "\n",
        "    // Host/CPU arrays\n",
        "    int *host_input_data = new int[n];\n",
        "    int *host_output_data = new int[(n + 255) / 256]; // to have sufficient size for output array\n",
        "\n",
        "    // Device/GPU arrays\n",
        "    int *dev_input_data, *dev_output_data;\n",
        "\n",
        "    // Init data\n",
        "    srand(42); // Fixed seed\n",
        "    for (int i = 0; i < n; i++){\n",
        "        host_input_data[i] = rand() % 100;\n",
        "    }\n",
        "\n",
        "    // Allocating memory on GPU for device arrays\n",
        "    cudaMalloc(&dev_input_data, bytes);\n",
        "    cudaMalloc(&dev_output_data, (n + 255) / 256 * sizeof(int));\n",
        "\n",
        "    // Copying our data onto the device (GPU)\n",
        "    cudaMemcpy(dev_input_data, host_input_data, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blockSize = 256; // number of threads per block\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now(); // start timer\n",
        "\n",
        "    // Launch Kernel and Synchronize threads\n",
        "    int num_blocks = (n + (2 * blockSize) - 1) / (2 * blockSize);   // Modifying this to account for the fact that 1 thread accesses 2 elements\n",
        "    cudaError_t err;\n",
        "    reduce4<<<num_blocks, blockSize, blockSize * sizeof(int)>>>(dev_input_data, dev_output_data);\n",
        "    err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err) << std::endl;\n",
        "    }\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start).count() / 1000.0; // duration in milliseconds with three decimal points\n",
        "\n",
        "    // Copying data back to the host (CPU)\n",
        "    cudaMemcpy(host_output_data, dev_output_data, (n + 255) / 256 * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on the host\n",
        "    int finalResult = host_output_data[0];\n",
        "    for (int i = 1; i < (n + 255) / 256; ++i) {\n",
        "        finalResult += host_output_data[i];\n",
        "    }\n",
        "\n",
        "    // CPU Summation for verification\n",
        "    int cpuResult = std::accumulate(host_input_data, host_input_data + n, 0);\n",
        "    if (cpuResult == finalResult) {\n",
        "        std::cout << \"\\033[32m\"; // Set text color to green\n",
        "        std::cout << \"Verification successful: GPU result matches CPU result.\\n\";\n",
        "        std::cout << \"GPU Result: \" << finalResult << \", CPU Result: \" << cpuResult << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"\\033[31m\"; // Set text color to red\n",
        "        std::cout << \"Verification failed: GPU result (\" << finalResult << \") does not match CPU result (\" << cpuResult << \").\\n\";\n",
        "        std::cout << \"GPU Result: \" << finalResult << \", CPU Result: \" << cpuResult << std::endl;\n",
        "    }\n",
        "    std::cout << \"\\033[0m\"; // Reset text color to default\n",
        "\n",
        "    double bandwidth = (duration > 0) ? (bytes / duration / 1e6) : 0; // computed in GB/s, handling zero duration\n",
        "    std::cout << \"Reduced result: \" << finalResult << std::endl;\n",
        "    std::cout << \"Time elapsed: \" << duration << \" ms\" << std::endl;\n",
        "    std::cout << \"Effective bandwidth: \" << bandwidth << \" GB/s\" << std::endl;\n",
        "\n",
        "    // Freeing memory\n",
        "    cudaFree(dev_input_data);\n",
        "    cudaFree(dev_output_data);\n",
        "    delete[] host_input_data;\n",
        "    delete[] host_output_data;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLiyPHaehv3x"
      },
      "outputs": [],
      "source": [
        "!nvcc -arch=sm_75 four.cu -o four"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3SquRZDiGT2",
        "outputId": "25ca9f27-1924-4086-f97d-fd3d302bbe36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32mVerification successful: GPU result matches CPU result.\n",
            "GPU Result: 207451054, CPU Result: 207451054\n",
            "\u001b[0mReduced result: 207451054\n",
            "Time elapsed: 0.289 ms\n",
            "Effective bandwidth: 58.0527 GB/s\n"
          ]
        }
      ],
      "source": [
        "!./four"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
