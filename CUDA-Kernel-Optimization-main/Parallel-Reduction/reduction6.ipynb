{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGX_tK1nofB-",
        "outputId": "21f60f2d-dc8b-4f38-b918-c08d25737985"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing six.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile six.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include<cuda_runtime.h>\n",
        "#include <chrono>\n",
        "#include <numeric>\n",
        "\n",
        "// Adding this function to help with unrolling and adding the Template\n",
        "template <unsigned int blockSize>\n",
        "__device__ void warpReduce(volatile int* sdata, unsigned int tid){\n",
        "    if(blockSize >= 64) sdata[tid] += sdata[tid + 32];\n",
        "    if(blockSize >= 32) sdata[tid] += sdata[tid + 16];\n",
        "    if(blockSize >= 16) sdata[tid] += sdata[tid + 8];\n",
        "    if(blockSize >= 8) sdata[tid] += sdata[tid + 4];\n",
        "    if(blockSize >= 4) sdata[tid] += sdata[tid + 2];\n",
        "    if(blockSize >= 2) sdata[tid] += sdata[tid + 1];\n",
        "}\n",
        "\n",
        "// REDUCTION 6 â€“ Multiple Adds / Threads\n",
        "template <int blockSize>\n",
        "__global__ void reduce6(int *g_in_data, int *g_out_data, unsigned int n){\n",
        "    extern __shared__ int sdata[];  // stored in the shared memory\n",
        "\n",
        "    // Each thread loading one element from global onto shared memory\n",
        "    unsigned int tid = threadIdx.x;\n",
        "    unsigned int i = blockIdx.x*(blockSize*2) + tid;\n",
        "    unsigned int gridSize = blockDim.x * 2 * gridDim.x;\n",
        "    sdata[tid] = 0;\n",
        "\n",
        "    while(i < n) {\n",
        "      sdata[tid] += g_in_data[i] + g_in_data[i + blockSize];\n",
        "      i += gridSize;\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Perform reductions in steps, reducing thread synchronization\n",
        "    if (blockSize >= 512) {\n",
        "        if (tid < 256) { sdata[tid] += sdata[tid + 256]; } __syncthreads();\n",
        "    }\n",
        "    if (blockSize >= 256) {\n",
        "        if (tid < 128) { sdata[tid] += sdata[tid + 128]; } __syncthreads();\n",
        "    }\n",
        "    if (blockSize >= 128) {\n",
        "        if (tid < 64) { sdata[tid] += sdata[tid + 64]; } __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid < 32) warpReduce<blockSize>(sdata, tid);\n",
        "\n",
        "    if (tid == 0){\n",
        "        g_out_data[blockIdx.x] = sdata[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// I hope to use this main file for all of the reduction files\n",
        "int main(){\n",
        "    int n = 1<<22; // Increase to about 4M elements\n",
        "    size_t bytes = n * sizeof(int);\n",
        "\n",
        "    // Host/CPU arrays\n",
        "    int *host_input_data = new int[n];\n",
        "    int *host_output_data = new int[(n + 255) / 256]; // to have sufficient size for output array\n",
        "\n",
        "    // Device/GPU arrays\n",
        "    int *dev_input_data, *dev_output_data;\n",
        "\n",
        "    // Init data\n",
        "    srand(42); // Fixed seed\n",
        "    for (int i = 0; i < n; i++){\n",
        "        host_input_data[i] = rand() % 100;\n",
        "    }\n",
        "\n",
        "    // Allocating memory on GPU for device arrays\n",
        "    cudaMalloc(&dev_input_data, bytes);\n",
        "    cudaMalloc(&dev_output_data, (n + 255) / 256 * sizeof(int));\n",
        "\n",
        "    // Copying our data onto the device (GPU)\n",
        "    cudaMemcpy(dev_input_data, host_input_data, bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int blockSize = 256; // number of threads per block\n",
        "    int num_blocks = (n + (2 * blockSize) - 1) / (2 * blockSize);   // Modifying this to account for the fact that 1 thread accesses 2 elements\n",
        "\n",
        "    auto start = std::chrono::high_resolution_clock::now(); // start timer\n",
        "\n",
        "    // Needed for Complete unrolling\n",
        "    // Launch Kernel and Synchronize threads\n",
        "    switch (blockSize) {\n",
        "        case 512:\n",
        "            reduce6<512><<<num_blocks, 512, 512 * sizeof(int)>>>(dev_input_data, dev_output_data, n);\n",
        "            break;\n",
        "        case 256:\n",
        "            reduce6<256><<<num_blocks, 256, 256 * sizeof(int)>>>(dev_input_data, dev_output_data, n);\n",
        "            break;\n",
        "        case 128:\n",
        "            reduce6<128><<<num_blocks, 128, 128 * sizeof(int)>>>(dev_input_data, dev_output_data, n);\n",
        "            break;\n",
        "    }\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    auto stop = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(stop - start).count() / 1000.0; // duration in milliseconds with three decimal points\n",
        "\n",
        "    // Copying data back to the host (CPU)\n",
        "    cudaMemcpy(host_output_data, dev_output_data, (n + 255) / 256 * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Final reduction on the host\n",
        "    int finalResult = host_output_data[0];\n",
        "    for (int i = 1; i < (n + 255) / 256; ++i) {\n",
        "        finalResult += host_output_data[i];\n",
        "    }\n",
        "\n",
        "    // CPU Summation for verification\n",
        "    int cpuResult = std::accumulate(host_input_data, host_input_data + n, 0);\n",
        "    if (cpuResult == finalResult) {\n",
        "        std::cout << \"\\033[32m\"; // Set text color to green\n",
        "        std::cout << \"Verification successful: GPU result matches CPU result.\\n\";\n",
        "        std::cout << \"GPU Result: \" << finalResult << \", CPU Result: \" << cpuResult << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"\\033[31m\"; // Set text color to red\n",
        "        std::cout << \"Verification failed: GPU result (\" << finalResult << \") does not match CPU result (\" << cpuResult << \").\\n\";\n",
        "        std::cout << \"GPU Result: \" << finalResult << \", CPU Result: \" << cpuResult << std::endl;\n",
        "    }\n",
        "    std::cout << \"\\033[0m\"; // Reset text color to default\n",
        "\n",
        "    double bandwidth = (duration > 0) ? (bytes / duration / 1e6) : 0; // computed in GB/s, handling zero duration\n",
        "    std::cout << \"Reduced result: \" << finalResult << std::endl;\n",
        "    std::cout << \"Time elapsed: \" << duration << \" ms\" << std::endl;\n",
        "    std::cout << \"Effective bandwidth: \" << bandwidth << \" GB/s\" << std::endl;\n",
        "\n",
        "    // Freeing memory\n",
        "    cudaFree(dev_input_data);\n",
        "    cudaFree(dev_output_data);\n",
        "    delete[] host_input_data;\n",
        "    delete[] host_output_data;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 six.cu -o six"
      ],
      "metadata": {
        "id": "L2WPdXpEon1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpSXXQDmotVR",
        "outputId": "ead82358-169d-4410-df17-077a5378a3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mVerification successful: GPU result matches CPU result.\n",
            "GPU Result: 207451054, CPU Result: 207451054\n",
            "\u001b[0mReduced result: 207451054\n",
            "Time elapsed: 0.284 ms\n",
            "Effective bandwidth: 59.0747 GB/s\n"
          ]
        }
      ]
    }
  ]
}